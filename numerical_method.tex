\newpage
\section{Numerical Method}
\subsection{Tree Method}
\subsubsection{Binary Tree}
Suppose we have a dynamic
\begin{equation}
\begin{aligned}
dX_t = \mu dt + \sigma dW_t
\end{aligned}
\end{equation}
For programming convenient, we choose $\Delta T = T/N$. Then we need to choose
$X_u$, $X_d$, $P$ (upward probablity) to be.
\begin{equation}
\begin{aligned}
P X_u - (1-P) X_d &= \mu \Delta T \\
P X_u^2 + (1-P) X_d^2 -\mu^2 \Delta T ^2 &= \sigma^2 \Delta T \leftarrow {\color{blue}Var(X) = E(X^2) - E(X)^2} \\
\end{aligned}
\end{equation}

If we want to further write our program to have $X_u$ equals $X_d$, {\color{red} then P is not flexible enough to match $\mu$ and $\sigma$ }

\subsubsection{Trinomial Tree}
To solve the issue above. We add a $P_m$ in addition to $P_u$ and $P_d$, where the sum of three is 1. The actual equation is trivial here. If the process exists unique hedges, then the method is ok, however, 3 state model itself does not provide a unique risk neutral measure framework.

\subsection{Finite Difference Method}
\subsection{Explicity Scheme}
Directly use future's x derivative:
\begin{equation}
\begin{aligned}
\frac{\partial C}{\partial t} &= \frac{C^j_{n+1} - C^j_{n}}{\Delta t} \\
\frac{\partial C}{\partial x} &= \frac{C^{j+1}_{n+1} - C^{j-1}_{n+1}}{2 \Delta x}\\
\frac{\partial^2 C}{\partial^2 x} &= \frac{C^{j+1}_{n+1} -2C^{j}_{n+1}+ C^{j-1}_{n+1}}{\Delta x ^2}
\end{aligned}
\end{equation}
Local truncation error $L(x,t) = O(\Delta t) + O(\Delta x)^2$. The stable condition is $\Delta t \leq \frac{1}{\sigma^2} (\Delta X)^2$

\subsection{Implicity Scheme}
Directly use past x derivative:
\begin{equation}
\begin{aligned}
\frac{\partial C}{\partial t} &= \frac{C^j_{n+1} - C^j_{n}}{\Delta t} \\
\frac{\partial C}{\partial x} &= \frac{C^{j+1}_{n} - C^{j-1}_{n}}{2 \Delta x}\\
\frac{\partial^2 C}{\partial^2 x} &= \frac{C^{j+1}_{n} -2C^{j}_{n}+ C^{j-1}_{n}}{\Delta x ^2}
\end{aligned}
\end{equation}
Local truncation error $L(x,t) = O(\Delta t) + O(\Delta x)^2$. The stable condition is for all

\subsection{Crank-Nicholson Scheme}
Directly use past x derivative:
\begin{equation}
\begin{aligned}
\frac{\partial C}{\partial t} &= \frac{C^j_{n+1} - C^j_{n}}{\Delta t} \\
\frac{\partial C}{\partial x} &= 0.5 \frac{C^{j+1}_{n} - C^{j-1}_{n}}{2 \Delta x} + 0.5 \frac{C^{j+1}_{n+1} - C^{j-1}_{n+1}}{2 \Delta x}\\
\frac{\partial^2 C}{\partial^2 x} &= 0.5 （\frac{C^{j+1}_{n} -2C^{j}_{n}+ C^{j-1}_{n}}{\Delta x ^2}） + 0.5 \frac{C^{j+1}_{n+1} -2C^{j}_{n+1}+ C^{j-1}_{n+1}}{\Delta x ^2}
\end{aligned}
\end{equation}
Local truncation error $L(x,t) = O(\Delta t)^2 + O(\Delta x)^2$. The stable condition is for all


\subsection{Monte-Carlo}
\subsubsection{Integration}
We can use monte-carlo for integration. Just find a density map $p(x)$
\begin{equation}
\begin{aligned}
 \int_\Omega f(x) dx &= E \frac{f(x)}{p(x)} p(x) dx \\
 \int_\Omega p(x) dx &= 1
\end{aligned}
\end{equation}

\subsubsection{Ways to Reduce Variance}
\begin{enumerate}
\item Antithetic way
\item Control variable $\hat{X} += Y - \hat{Y}$
\item Importance Sample $\frac{h(x)f(x)}{g(x)}$ reduce $E(h(x)^2)$
\end{enumerate}

\subsubsection{Longstaff-Schwartz}
\begin{enumerate}
\item generate N path using random (np.random.randn(N, T))
\item backward for loop, each t look at the result of t+1, group the payoff by state that you define (S stock price in the simple case).
\item do the regression (Y is payoff X is state) and see if the intrinsic value is better than the expected future payoff of the current state.
\item get the current state decision and payoff
\end{enumerate}
